{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea726bb-b38c-4277-ab0b-2a18711e110b",
   "metadata": {},
   "source": [
    "## Analyze parsed metadata from nionswift to improve mapping onto NXem for the nxs_nion parser of pynxtools-em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f0874-cc85-4cdd-93b4-32f9a5c60be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "\n",
    "print(f\"{os.getcwd()}\")\n",
    "\n",
    "directory = \"CHANGEME\".replace(\"/\", f\"{os.path.sep}\")\n",
    "print(directory)\n",
    "# serialized_axes = \"[{'offset': -0.07421380365287618, 'scale': 7.247441762976189e-05, 'units': 'rad'}, {'offset': -0.07421380365287618, 'scale': 7.247441762976189e-05, 'units': 'rad'}]\"\n",
    "# serialized_dims = \"(1,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81147a03-92d8-4637-b5c6-91b96c10d96e",
   "metadata": {},
   "source": [
    "## Variants of NXdata taken and how to interpret these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec9bf6-256c-4e08-acf6-ee18b567136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_to_list_of_axes(serialized: str) -> list[dict[str, float | int | str]]:\n",
    "    \"\"\"Deserialize into a list of dictionaries each representing a physical dimension axis.\"\"\"\n",
    "    deserialized = ast.literal_eval(serialized)\n",
    "    any_dict_not_an_axis = False\n",
    "    for obj in deserialized:\n",
    "        if isinstance(obj, dict):\n",
    "            if (\n",
    "                all(key in obj for key in (\"offset\", \"scale\", \"units\"))\n",
    "                and len(obj.keys()) == 3\n",
    "            ):\n",
    "                continue\n",
    "        any_dict_not_an_axis = True\n",
    "    if not any_dict_not_an_axis:\n",
    "        return deserialized\n",
    "    return []\n",
    "\n",
    "\n",
    "# print(deserialize_to_list_of_axes(serialized_axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbcd15-18cf-4728-a659-fe10d38f385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_to_tuple_of_dimensions(serialized: str) -> tuple[int]:\n",
    "    \"\"\"Deserialize to a typical return value as of np.shape().\"\"\"\n",
    "    deserialized = ast.literal_eval(serialized)\n",
    "    if isinstance(deserialized, tuple):\n",
    "        if all(isinstance(value, int) for value in deserialized):\n",
    "            return deserialized\n",
    "    return ()\n",
    "\n",
    "\n",
    "# print(deserialize_to_tuple_of_dimensions(serialized_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae24769-2225-409c-a5e5-8de5f9bb4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_axes(list_of_axes: list[dict[str, float | int | str]]) -> str:\n",
    "    \"\"\"Create a token of the sorted axes to identify the sub-space in phase space.\"\"\"\n",
    "    token = []\n",
    "    for axis in list_of_axes:\n",
    "        if axis[\"units\"] == \"\":\n",
    "            token.append(\"unitless\")\n",
    "        else:\n",
    "            token.append(axis[\"units\"])\n",
    "    return \";\".join(token)\n",
    "\n",
    "\n",
    "# print(tokenize_axes(deserialize_to_list_of_axes(serialized_axes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a84ac-2fbf-460a-b066-6508cf176a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logfiles = {}\n",
    "stats_axes = {}  # statistics of how many datasets use specific combinations of physical dimension axes in sequence\n",
    "for root, dirs, files in os.walk(f\"{directory}{os.sep}log\"):\n",
    "    for file in files:\n",
    "        fpath = f\"{root}/{file}\".replace(os.sep * 2, os.sep)\n",
    "        if not fpath.endswith(\".log\"):\n",
    "            continue\n",
    "        print(fpath)\n",
    "        with open(fpath) as fp:\n",
    "            for dirty_line in fp.readlines():\n",
    "                clean_line = dirty_line.strip()\n",
    "                pattern = r\"^[A-Z]+\\s+\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.[+-]\\d{4} ndata, metadata\\.json, flat___dimensional_calibrations___*?(\\[(?:\\s*\\{[^{}]*?:[^{}]*?\\}\\s*,?)+\\])\"\n",
    "                match = re.search(pattern, clean_line)\n",
    "                if match:\n",
    "                    keyword = tokenize_axes(\n",
    "                        deserialize_to_list_of_axes(f\"{match.group(1)}\")\n",
    "                    )\n",
    "                    if keyword in stats_axes:\n",
    "                        stats_axes[keyword] += 1\n",
    "                    else:\n",
    "                        stats_axes[keyword] = 1\n",
    "                    del keyword\n",
    "                del match, pattern, clean_line\n",
    "        continue\n",
    "        # project_log_fpath, mime_type = fpath.split(\".\")\n",
    "        # if project_log_fpath not in logfiles:\n",
    "        #     logfiles[project_log_fpath] = {\"log\": project_log_fpath}\n",
    "        # else:\n",
    "        #     raise KeyError(f\"Such a project_log_fpath exists already!\")\n",
    "print(\"Batch processing successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a1f9149-0fce-4d0e-8c20-18a20dd51ce6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9694\t0.388\tnm;nm\n",
      "4138\t0.5537\teV\n",
      "3049\t0.6757\trad;rad\n",
      "2677\t0.7829\tunitless;eV\n",
      "1140\t0.8285\tunitless;nm;nm\n",
      "936\t0.866\tnm;nm;eV\n",
      "649\t0.892\tunitless\n",
      "561\t0.9144\trad;eV\n",
      "341\t0.9281\tunitless;unitless\n",
      "327\t0.9412\t1/nm;1/nm\n",
      "276\t0.9522\teV;nm;nm\n",
      "207\t0.9605\tnm\n",
      "202\t0.9686\trad\n",
      "194\t0.9763\tunitless;unitless;eV\n",
      "137\t0.9818\tunitless;rad;eV\n",
      "132\t0.9871\tunitless;unitless;unitless\n",
      "115\t0.9917\titeration\n",
      "59\t0.9941\tnm;nm;rad;eV\n",
      "51\t0.9961\tunitless;nm;nm;eV\n",
      "33\t0.9974\tnm;nm;unitless;eV\n",
      "14\t0.998\t1/;1/\n",
      "12\t0.9985\tunitless;nm\n",
      "8\t0.9988\tnm;eV\n",
      "7\t0.9991\tunitless;unitless;unitless;unitless\n",
      "7\t0.9994\tnm;nm;rad;rad\n",
      "6\t0.9996\tnm;rad;eV\n",
      "4\t0.9998\t1/nm\n",
      "2\t0.9998\t1/rad;1/rad\n",
      "1\t0.9999\tunitless;unitless;unitless;unitless;unitless\n",
      "1\t0.9999\tunitless;rad;rad\n",
      "1\t1.0\t1/eV\n",
      "1\t1.0\tnm;nm;rad\n"
     ]
    }
   ],
   "source": [
    "pivot_dims = []\n",
    "total_cnts = 0\n",
    "for key, cnts in stats_axes.items():\n",
    "    pivot_dims.append((cnts, key))\n",
    "    total_cnts += cnts\n",
    "pivot_dims.sort(key=lambda x: x[0], reverse=True)\n",
    "sum_cnts = 0\n",
    "for cnts, key in pivot_dims:\n",
    "    sum_cnts += cnts\n",
    "    print(f\"{cnts}\\t{np.around(sum_cnts / total_cnts, decimals=4)}\\t{key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
